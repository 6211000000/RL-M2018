{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - codes with explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------\n",
    "#Q2)\n",
    "\n",
    "state-value function corresponding to the given equiprobable policy, was found by solving linear system of bellman equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.30899634  8.78929186  4.42761918  5.32236759  1.49217876  1.52158807\n",
      "  2.99231786  2.25013995  1.9075717   0.54740271  0.05082249  0.73817059\n",
      "  0.67311326  0.35818621 -0.40314114 -0.9735923  -0.43549543 -0.35488227\n",
      " -0.58560509 -1.18307508 -1.85770055 -1.34523126 -1.22926726 -1.42291815\n",
      " -1.97517905]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#################setup\n",
    "\n",
    "#actions are in this order: Up, Down, Left, Right\n",
    "\n",
    "rewards=[0,-1,10,5] #possible rewards at each state\n",
    "\n",
    "p=[] #p[present state][actions][next state][rewards] \n",
    "for i in range(25):#s(0-24)\n",
    "    hold=[]\n",
    "    for j in range(4):#a(nswe)\n",
    "        l=[]\n",
    "        for k in range(25):#s'(0-24)\n",
    "            m=[0 for o in range(4)] #(0,-1,10,5)\n",
    "            l.append(m)            \n",
    "        hold.append(l)\n",
    "    p.append(hold)\n",
    "    \n",
    "        \n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        if(i==3):\n",
    "            p[i][j][13][3]=1\n",
    "        elif(i==1):\n",
    "            p[i][j][21][2]=1\n",
    "        else:    \n",
    "            if(j==0):\n",
    "                if(i-5<0):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-5][0]=1\n",
    "            elif(j==1):\n",
    "                if(i+5>24):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+5][0]=1\n",
    "            elif(j==2):\n",
    "                if(i==0 or i==5 or i==10 or i==15 or i==20):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-1][0]=1   \n",
    "            elif(j==3):\n",
    "                if(i==4 or i==9 or i==14 or i==19 or i==24):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+1][0]=1\n",
    "                \n",
    "        \n",
    "        \n",
    "#just verification of the MDP setup\n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        su=0;count=1;\n",
    "        for k in range(len(p[i][j])):\n",
    "            for o in range(len(p[i][j][k])):\n",
    "#                 print(k,o,p[i][j][k][o],count);count+=1\n",
    "                su+=p[i][j][k][o]\n",
    "#         if(su!=1):\n",
    "#             print(i,j,su)\n",
    "        assert(su==1)\n",
    "        \n",
    "\n",
    "#policy - equiprobable\n",
    "policy=[] #policy[present state][actions]\n",
    "for i in range(25):#present states\n",
    "    given_State=[]\n",
    "    for i in range(4):#actions\n",
    "        given_State.append(0.25)\n",
    "    policy.append(given_State)\n",
    "\n",
    "gamma=0.9 #discounting\n",
    "    \n",
    "####################solving system of linear equations for bellman equations. Ax=b, x=state_value\n",
    "b=[0 for i in range(25)]\n",
    "\n",
    "for i in range(25):#present state\n",
    "    for j in range(4):#actions\n",
    "        for k in range(25):#next state\n",
    "            for o in range(4):#rewards\n",
    "                b[i]-=(rewards[o]*p[i][j][k][o]*policy[i][j])\n",
    "\n",
    "A=[]\n",
    "\n",
    "for i in range(25):#present state\n",
    "    given_coeff=[]\n",
    "    for j in range(25):#next state\n",
    "        su=0\n",
    "        for k in range(4):#actions\n",
    "            for o in range(4):#rewards\n",
    "                su+=(gamma*p[i][k][j][o]*policy[i][k])\n",
    "        given_coeff.append(su)\n",
    "        if(j==i):#adjusting the coeffs of the states that are on the LHS\n",
    "            given_coeff[-1]-=1\n",
    "    A.append(given_coeff)\n",
    "\n",
    "        \n",
    "A=np.array(A);b=np.array(b)\n",
    "state_value=np.linalg.solve(A, b) #linear algebra solved using numpy\n",
    "\n",
    "print(state_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------\n",
    "#Q4)\n",
    "\n",
    "This section uses policy iteration to solve system of non-linear equations. Policy Iteration has two functions:\n",
    "\n",
    "1>pol_eval():\n",
    "\n",
    "INPUT: p,policy,state_value,gamma,epsilon,rewards\n",
    "p - MDP setup\n",
    "policy - current policy\n",
    "state_value - current state_values\n",
    "gamma - discounting\n",
    "epsilon - error margin\n",
    "rewards - list of possible rewards at each state\n",
    "\n",
    "OUTPUT: calls pol_improve()\n",
    "\n",
    "2>pol_improve():\n",
    "\n",
    "INPUT: same as pol_eval()\n",
    "\n",
    "OUTPUT: calls pol_eval() or returns the final policy and the corresponding final state_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final policy\n",
      "[[0.0, 0.0, 0.0, 1.0], [0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0], [0.5, 0.0, 0.5, 0.0]]\n",
      "\n",
      "final state values\n",
      "[21.977485287046154, 24.419428096830984, 21.977485287147886, 19.419428096830984, 17.477485287147886, 19.77973675834154, 21.977485287147886, 19.7797367584331, 17.80176308258979, 16.02158677433081, 17.801763082507385, 19.7797367584331, 17.80176308258979, 16.02158677433081, 14.419428096897729, 16.021586774256647, 17.80176308258979, 16.02158677433081, 14.419428096897729, 12.977485287207957, 14.419428096830982, 16.02158677433081, 14.419428096897729, 12.977485287207957, 11.679736758487161]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#######################setup\n",
    "\n",
    "#actions are in this order: Up, Down, Left, Right\n",
    "\n",
    "rewards=[0,-1,10,5] #possible rewards at each state\n",
    "\n",
    "p=[] #p[present state][actions][next state][rewards]\n",
    "for i in range(25):#s(0-24)\n",
    "    hold=[]\n",
    "    for j in range(4):#a(nswe)\n",
    "        l=[]\n",
    "        for k in range(25):#s'(0-24)\n",
    "            m=[0 for o in range(4)] #(0,-1,10,5)\n",
    "            l.append(m)            \n",
    "        hold.append(l)\n",
    "    p.append(hold)\n",
    "    \n",
    "        \n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        if(i==3):\n",
    "            p[i][j][13][3]=1\n",
    "        elif(i==1):\n",
    "            p[i][j][21][2]=1\n",
    "        else:    \n",
    "            if(j==0):\n",
    "                if(i-5<0):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-5][0]=1\n",
    "            elif(j==1):\n",
    "                if(i+5>24):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+5][0]=1\n",
    "            elif(j==2):\n",
    "                if(i==0 or i==5 or i==10 or i==15 or i==20):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-1][0]=1   \n",
    "            elif(j==3):\n",
    "                if(i==4 or i==9 or i==14 or i==19 or i==24):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+1][0]=1\n",
    "                \n",
    "        \n",
    "        \n",
    "#just verification of the MDP setup\n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        su=0;count=1;\n",
    "        for k in range(len(p[i][j])):\n",
    "            for o in range(len(p[i][j][k])):\n",
    "#                 print(k,o,p[i][j][k][o],count);count+=1\n",
    "                su+=p[i][j][k][o]\n",
    "#         if(su!=1):\n",
    "#             print(i,j,su)\n",
    "        assert(su==1)\n",
    "        \n",
    "\n",
    "#policy initialization - equiprobable\n",
    "policy=[] #policy[present state][actions] \n",
    "for i in range(25):#present states\n",
    "    given_State=[]\n",
    "    for i in range(4):#actions\n",
    "        given_State.append(0.25)\n",
    "    policy.append(given_State)\n",
    "\n",
    "gamma=0.9 #discounting\n",
    "    \n",
    "###########################solving the system using policy iteration\n",
    "        \n",
    "epsilon=0.00001 #convergence tolerance\n",
    "    \n",
    "state_value=[random.random() for i in range(25)]#initialization\n",
    "    \n",
    "\n",
    "#policy evaluation\n",
    "def pol_eval(p,policy,state_value,gamma,epsilon,rewards): #make these globals, function parameters\n",
    "    while(True):\n",
    "        \n",
    "        delta=-math.inf #delta for convergence checking\n",
    "        for i in range(25):#present state\n",
    "            old=state_value[i]\n",
    "\n",
    "            su=0\n",
    "            for o in range(4):#actions\n",
    "                for j in range(25):#next states\n",
    "                    for k in range(4):#rewards\n",
    "                        su+=(rewards[k]+(gamma*state_value[j]))*p[i][o][j][k]*policy[i][o]\n",
    "            \n",
    "            state_value[i]=su #state value of state i update in place\n",
    "            \n",
    "            delta=max(delta,abs(old-state_value[i]))\n",
    "        \n",
    "        if(delta<epsilon):\n",
    "            break\n",
    "        \n",
    "    return pol_improve(p,policy,state_value,gamma,epsilon,rewards)\n",
    "        \n",
    "#policy improvement\n",
    "def pol_improve(p,policy,state_value,gamma,epsilon,rewards):\n",
    "    old_pol=copy.deepcopy(policy)\n",
    "    \n",
    "    for i in range(25):#present state\n",
    "        ac_values=[0 for i in range(4)] #action values calculated\n",
    "        for j in range(4):#actions\n",
    "            for k in range(25):#next state\n",
    "                for o in range(4):#rewards\n",
    "                    ac_values[j]+=p[i][j][k][o]*(rewards[o]+(gamma*state_value[k]))\n",
    "\n",
    "        #only all optimal actions assigned equal non-zero probabilities\n",
    "        act=np.argmax(ac_values)\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]=0\n",
    "            if(ac_values[act]==ac_values[j]):\n",
    "                policy[i][j]=1\n",
    "\n",
    "        su=sum(policy[i])\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]/=su\n",
    "    \n",
    "    if(policy!=old_pol):\n",
    "        return pol_eval(p,policy,state_value,gamma,epsilon,rewards)\n",
    "    else:\n",
    "        return [policy, state_value] \n",
    "        \n",
    "\n",
    "        \n",
    "[new_policy, new_state_value]=pol_eval(p,policy,state_value,gamma,epsilon,rewards);\n",
    "        \n",
    "print();print(\"final policy\");print(new_policy);print();print(\"final state values\");print(new_state_value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------\n",
    "#Q6)both the iterations\n",
    "\n",
    "policy iteration code is almost same as that in Q4, just the bug has been fixed here\n",
    "\n",
    "Bug in the policy iteration: it may keep infintely switching between multiple optimal policies and hence never terminate(this was the policy iteration implemented in Q4). But the optimal state_values corresponding to any opitmal policy will be same. So we can just compare and check whether we have arrived at the optimal state_values or not(by checking whether the state_values have changed or not), and if yes then terminate the code from policy evaluation step itself.\n",
    "\n",
    "For Value iteration: function is val_loop(): Its input and output are same as that for the functions of policy iteration\n",
    "\n",
    "\n",
    "Logging: an \"iteration\" variable introduced to properly log the policy and state_values at each iteration.\n",
    "Also the final policy and state_values are also shown.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "#######################setup\n",
    "\n",
    "#actions are in this order: Up, Down, Left, Right\n",
    "\n",
    "rewards=[0,-1] #possible rewards at each state\n",
    "\n",
    "p=[] #p[present state][actions][next state][rewards]\n",
    "for i in range(16):#s(0-15: 0, 15 are terminal states)\n",
    "    hold=[]\n",
    "    for j in range(4):#a(nswe)\n",
    "        l=[]\n",
    "        for k in range(16):#s'(0-15)\n",
    "            m=[0 for o in range(2)] #r(0,-1)\n",
    "            l.append(m)            \n",
    "        hold.append(l)\n",
    "    p.append(hold)\n",
    "    \n",
    "        \n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        if(i==0):\n",
    "            p[i][j][i][0]=1\n",
    "        elif(i==15):\n",
    "            p[i][j][i][0]=1\n",
    "        else:    \n",
    "            if(j==0):\n",
    "                if(i-4<0):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-4][1]=1\n",
    "            elif(j==1):\n",
    "                if(i+4>15):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+4][1]=1\n",
    "            elif(j==2):\n",
    "                if(i==0 or i==4 or i==8 or i==12):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i-1][1]=1   \n",
    "            elif(j==3):\n",
    "                if(i==3 or i==7 or i==11 or i==15):\n",
    "                    p[i][j][i][1]=1\n",
    "                else:\n",
    "                    p[i][j][i+1][1]=1\n",
    "                \n",
    "        \n",
    "        \n",
    "#just verification of the MDP setup\n",
    "for i in range(len(p)):\n",
    "    for j in range(len(p[i])):\n",
    "        su=0;count=1;\n",
    "        for k in range(len(p[i][j])):\n",
    "            for o in range(len(p[i][j][k])):\n",
    "#                 print(k,o,p[i][j][k][o],count);count+=1\n",
    "                su+=p[i][j][k][o]\n",
    "#         if(su!=1):\n",
    "#             print(i,j,su)\n",
    "        assert(su==1)\n",
    "        \n",
    "\n",
    "#policy initialization - equiprobable\n",
    "policy=[] #policy[present state][actions] \n",
    "for i in range(16):#present states\n",
    "    given_State=[]\n",
    "    for i in range(4):#actions\n",
    "        given_State.append(0.25)\n",
    "    policy.append(given_State)\n",
    "\n",
    "gamma=1 #discounting\n",
    "    \n",
    "###########################iterations setup\n",
    "        \n",
    "epsilon=0.00001 #convergence tolerance\n",
    "    \n",
    "state_value=[random.random() for i in range(16)]#initialization\n",
    "state_value[0]=0;state_value[15]=0#terminal states\n",
    "    \n",
    "###########value iteration\n",
    "\n",
    "def val_loop(p,state_value,gamma,epsilon,rewards,policy,iteration):\n",
    "    while(True):\n",
    "        \n",
    "        delta=-math.inf #delta for convergence checking\n",
    "        for i in range(16):#present state\n",
    "            old=state_value[i]\n",
    "\n",
    "            temp=-math.inf\n",
    "            for o in range(4):#actions\n",
    "                su=0;\n",
    "                for j in range(16):#next states\n",
    "                    for k in range(2):#rewards\n",
    "                        su+=(rewards[k]+(gamma*state_value[j]))*p[i][o][j][k]\n",
    "                temp=max(temp,su)\n",
    "            state_value[i]=temp\n",
    "            delta=max(delta,abs(old-state_value[i]))\n",
    "        \n",
    "        print(\"iteration\",iteration,\"value iter\")#for output logging\n",
    "        print(state_value)\n",
    "        iteration+=1\n",
    "        \n",
    "        if(delta<epsilon):\n",
    "            break\n",
    "    \n",
    "    #optimal policy generation based on the optimal state_values\n",
    "    for i in range(16):#present states\n",
    "        ac_values=[0 for i in range(4)] #action values calculated\n",
    "        for j in range(4):#actions\n",
    "            for k in range(16):#next state\n",
    "                for o in range(2):#rewards\n",
    "                    ac_values[j]+=p[i][j][k][o]*(rewards[o]+(gamma*state_value[k]))\n",
    "\n",
    "        #only all optimal actions assigned equal non-zero probabilities\n",
    "        act=np.argmax(ac_values)\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]=0\n",
    "            if(ac_values[act]==ac_values[j]):\n",
    "                policy[i][j]=1\n",
    "        \n",
    "        su=sum(policy[i])\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]/=su\n",
    "    \n",
    "    return [policy,state_value]\n",
    "\n",
    "\n",
    "\n",
    "###########policy iteration\n",
    "        \n",
    "#policy evaluation\n",
    "def pol_eval(p,policy,state_value,gamma,epsilon,rewards,iteration): #make these globals, function parameters\n",
    "    prev=copy.deepcopy(state_value) #bug of 4.4 fixed\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        delta=-math.inf #delta for convergence checking\n",
    "        for i in range(16):#present state\n",
    "            old=state_value[i]\n",
    "\n",
    "            su=0\n",
    "            for o in range(4):#actions\n",
    "                for j in range(16):#next states\n",
    "                    for k in range(2):#rewards\n",
    "                        su+=(rewards[k]+(gamma*state_value[j]))*p[i][o][j][k]*policy[i][o]\n",
    "            \n",
    "            state_value[i]=su #state value of state i update in place\n",
    "            \n",
    "            delta=max(delta,abs(old-state_value[i]))\n",
    "        \n",
    "        if(delta<epsilon):\n",
    "            break\n",
    "    \n",
    "    print(\"iteration\",iteration,\"policy eval\")#for output logging\n",
    "    print(state_value)\n",
    "    \n",
    "    if(prev==state_value): #bug of 4.4 fixed\n",
    "        return [policy, state_value]\n",
    "    else:  \n",
    "        return pol_improve(p,policy,state_value,gamma,epsilon,rewards,iteration)\n",
    "        \n",
    "        \n",
    "#policy improvement\n",
    "def pol_improve(p,policy,state_value,gamma,epsilon,rewards,iteration):\n",
    "    \n",
    "    old_pol=copy.deepcopy(policy)\n",
    "    for i in range(16):#present state\n",
    "        ac_values=[0 for i in range(4)] #action values calculated\n",
    "        for j in range(4):#actions\n",
    "            for k in range(16):#next state\n",
    "                for o in range(2):#rewards\n",
    "                    ac_values[j]+=p[i][j][k][o]*(rewards[o]+(gamma*state_value[k]))\n",
    "        \n",
    "        #only all optimal actions assigned equal non-zero probabilities\n",
    "        act=np.argmax(ac_values)\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]=0\n",
    "            if(ac_values[act]==ac_values[j]):\n",
    "                policy[i][j]=1\n",
    "\n",
    "        su=sum(policy[i])\n",
    "        for j in range(4):#actions\n",
    "            policy[i][j]/=su\n",
    "    \n",
    "    print(\"iteration\",iteration,\"policy improve\")#for output logging\n",
    "    print(policy)\n",
    "    iteration+=1\n",
    "    \n",
    "    if(policy!=old_pol):\n",
    "        return pol_eval(p,policy,state_value,gamma,epsilon,rewards,iteration)\n",
    "    else:\n",
    "        return [policy, state_value]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 value iter\n",
      "[0.0, -0.24904840990057087, -0.2843596783601786, -0.6013357983980395, -0.15119334992519662, -0.479866565031262, -0.4591688981999923, -0.6013357983980395, -0.15119334992519662, -0.4591688981999923, -0.32448374615594144, -0.9657379287718901, -0.6403902850247379, -0.32448374615594144, -0.32448374615594144, 0.0]\n",
      "iteration 1 value iter\n",
      "[0.0, -1.0, -1.2843596783601785, -1.6013357983980394, -1.0, -1.4591688981999922, -1.3244837461559413, -1.6013357983980394, -1.1511933499251965, -1.3244837461559413, -1.3244837461559413, -1.0, -1.3244837461559413, -1.3244837461559413, -1.0, 0.0]\n",
      "iteration 2 value iter\n",
      "[0.0, -1.0, -2.0, -2.6013357983980394, -1.0, -2.0, -2.3244837461559413, -2.0, -2.0, -2.3244837461559413, -2.0, -1.0, -2.3244837461559413, -2.0, -1.0, 0.0]\n",
      "iteration 3 value iter\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n",
      "iteration 4 value iter\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n",
      "\n",
      "final policy\n",
      "[[0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.5, 0.5, 0.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 0.5, 0.0, 0.5], [0.0, 1.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.25, 0.25, 0.25, 0.25]]\n",
      "\n",
      "final state values\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#Q6) value iterations [before running this block. run cell third from the top]\n",
    "[new_policy, new_state_value]=val_loop(p,state_value,gamma,epsilon,rewards,policy,0)\n",
    "\n",
    "print();print(\"final policy\");print(new_policy);print();print(\"final state values\");print(new_state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 policy eval\n",
      "[0.0, -13.999934050491271, -19.999905188756806, -21.999895642651044, -13.999934050491271, -17.999919071299644, -19.999912130028225, -19.99991313476825, -19.999905188756806, -19.999912130028225, -17.99992585383258, -13.99994464155944, -21.99989564265104, -19.99991313476825, -13.99994464155944, 0.0]\n",
      "iteration 0 policy improve\n",
      "[[0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.5, 0.0, 0.5], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.25, 0.25, 0.25, 0.25]]\n",
      "iteration 1 policy eval\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n",
      "iteration 1 policy improve\n",
      "[[0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.5, 0.5, 0.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 0.5, 0.0, 0.5], [0.0, 1.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.25, 0.25, 0.25, 0.25]]\n",
      "iteration 2 policy eval\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n",
      "\n",
      "final policy\n",
      "[[0.25, 0.25, 0.25, 0.25], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.5, 0.5, 0.0], [1.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.5, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.25, 0.25, 0.25, 0.25], [0.0, 0.5, 0.0, 0.5], [0.0, 1.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.25, 0.25, 0.25, 0.25]]\n",
      "\n",
      "final state values\n",
      "[0.0, -1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#Q6) policy iterations [before running this block. run cell third from the top]\n",
    "[new_policy, new_state_value]=pol_eval(p,policy,state_value,gamma,epsilon,rewards,0);\n",
    "   \n",
    "print();print(\"final policy\");print(new_policy);print();print(\"final state values\");print(new_state_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
